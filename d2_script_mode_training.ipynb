{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download, decompress the data\n",
    "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip \n",
    "!unzip balloon_dataset.zip\n",
    "!rm -r __MACOSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = \"us-east-2\"\n",
    "prefix_input = 'detectron2-input'\n",
    "prefix_output = 'detectron2-ouput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to upload it to S3\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=region)\n",
    "root_path = os.getcwd()\n",
    "data_path = os.path.join(root_path, \"balloon\")\n",
    "\n",
    "for path, subdirs, files in os.walk(data_path):\n",
    "    directory_name = path.replace(root_path+\"/\",\"\")\n",
    "    for file in files:\n",
    "        #print(os.path.join(root_path, directory_name, file))\n",
    "        s3_resource.Bucket(bucket).upload_file(os.path.join(root_path, directory_name, file), directory_name+'/'+file)\n",
    "        #print(f\"file {file} has been uploaded to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "Let's review the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.utils.logger\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m setup_logger\n",
      "setup_logger()\n",
      "\n",
      "\u001b[37m# import some common libraries\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcv2\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# import some common detectron2 utilities\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m model_zoo\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.engine\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DefaultPredictor\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.config\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_cfg\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.utils.visualizer\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Visualizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.data\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MetadataCatalog\n",
      "\n",
      "\n",
      "\u001b[37m# packages neededs for custom dataset\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.structures\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BoxMode\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2.data\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DatasetCatalog\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m():\n",
      "    cfg = get_cfg()\n",
      "    cfg.merge_from_file(model_zoo.get_config_file(\u001b[33m\"\u001b[39;49;00m\u001b[33mCOCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    cfg.DATASETS.TRAIN = (\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon_train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,)\n",
      "    cfg.DATASETS.TEST = ()\n",
      "    cfg.DATALOADER.NUM_WORKERS = \u001b[34m2\u001b[39;49;00m\n",
      "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\u001b[33m\"\u001b[39;49;00m\u001b[33mCOCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)  \u001b[37m# Let training initialize from model zoo\u001b[39;49;00m\n",
      "    cfg.SOLVER.IMS_PER_BATCH = \u001b[34m2\u001b[39;49;00m\n",
      "    cfg.SOLVER.BASE_LR = \u001b[34m0.00025\u001b[39;49;00m  \u001b[37m# pick a good LR\u001b[39;49;00m\n",
      "    cfg.SOLVER.MAX_ITER = \u001b[34m300\u001b[39;49;00m    \u001b[37m# 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\u001b[39;49;00m\n",
      "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = \u001b[34m128\u001b[39;49;00m   \u001b[37m# faster, and good enough for this toy dataset (default: 512)\u001b[39;49;00m\n",
      "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = \u001b[34m1\u001b[39;49;00m  \u001b[37m# only has one class (ballon)\u001b[39;49;00m\n",
      "    cfg.OUTPUT_DIR = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] \u001b[37m# TODO check that this config works fine\u001b[39;49;00m\n",
      "\n",
      "    trainer = DefaultTrainer(cfg) \n",
      "    trainer.resume_or_load(resume=\u001b[36mFalse\u001b[39;49;00m)\n",
      "    trainer.train()\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprepare_dataset\u001b[39;49;00m():\n",
      "    \u001b[34mfor\u001b[39;49;00m d \u001b[35min\u001b[39;49;00m [\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]:\n",
      "        DatasetCatalog.register(\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + d, \u001b[34mlambda\u001b[39;49;00m d=d: get_balloon_dicts(os.environ[f\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNELS_{d}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "        MetadataCatalog.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + d).set(thing_classes=[\u001b[33m\"\u001b[39;49;00m\u001b[33mballoon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_balloon_dicts\u001b[39;49;00m(img_dir):\n",
      "    \n",
      "    json_file = os.path.join(img_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mvia_region_data.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(json_file) \u001b[34mas\u001b[39;49;00m f:\n",
      "        imgs_anns = json.load(f)\n",
      "\n",
      "    dataset_dicts = []\n",
      "    \u001b[34mfor\u001b[39;49;00m idx, v \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(imgs_anns.values()):\n",
      "        record = {}\n",
      "        \n",
      "        filename = os.path.join(img_dir, v[\u001b[33m\"\u001b[39;49;00m\u001b[33mfilename\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "        height, width = cv2.imread(filename).shape[:\u001b[34m2\u001b[39;49;00m]\n",
      "        \n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mfile_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = filename\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = idx\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mheight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = height\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mwidth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = width\n",
      "      \n",
      "        annos = v[\u001b[33m\"\u001b[39;49;00m\u001b[33mregions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        objs = []\n",
      "        \u001b[34mfor\u001b[39;49;00m _, anno \u001b[35min\u001b[39;49;00m annos.items():\n",
      "            \u001b[34massert\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mregion_attributes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            anno = anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape_attributes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            px = anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mall_points_x\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            py = anno[\u001b[33m\"\u001b[39;49;00m\u001b[33mall_points_y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            poly = [(x + \u001b[34m0.5\u001b[39;49;00m, y + \u001b[34m0.5\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m x, y \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(px, py)]\n",
      "            poly = [p \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m poly \u001b[34mfor\u001b[39;49;00m p \u001b[35min\u001b[39;49;00m x]\n",
      "\n",
      "            obj = {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mbbox\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mbbox_mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: BoxMode.XYXY_ABS,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33msegmentation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: [poly],\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mcategory_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33miscrowd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m\n",
      "            }\n",
      "            objs.append(obj)\n",
      "        record[\u001b[33m\"\u001b[39;49;00m\u001b[33mannotations\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = objs\n",
      "        dataset_dicts.append(record)\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset_dicts\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    prepare_dataset()\n",
      "    train()\n"
     ]
    }
   ],
   "source": [
    "! pygmentize d2_script/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 20:25:37 Starting - Starting the training job...\n",
      "2020-04-06 20:25:39 Starting - Launching requested ML instances...\n",
      "2020-04-06 20:26:36 Starting - Preparing the instances for training.........\n",
      "2020-04-06 20:27:50 Downloading - Downloading input data...\n",
      "2020-04-06 20:28:25 Training - Downloading the training image......\n",
      "2020-04-06 20:29:38 Training - Training image download completed. Training in progress..\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-04-06 20:29:35,157 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-04-06 20:29:35,181 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-04-06 20:29:39,591 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-04-06 20:29:39,616 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-04-06 20:29:41,029 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-04-06 20:29:41,405 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-04-06 20:29:41,925 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-04-06 20:29:41,925 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-04-06 20:29:41,925 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-04-06 20:29:41,926 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34m2020-04-06 20:29:41,573 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-06 20:29:41,573 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-06 20:29:41,574 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-06 20:29:41,574 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpi41yrs8k/module_dir\u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmp5d4cq1n0/module_dir\u001b[0m\n",
      "\u001b[35mCollecting git+https://github.com/facebookresearch/detectron2.git (from -r requirements.txt (line 5))\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-7w2joi4d\n",
      "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-7w2joi4d\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/facebookresearch/detectron2.git (from -r requirements.txt (line 5))\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-ddelnpft\n",
      "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-ddelnpft\u001b[0m\n",
      "\u001b[34mCollecting termcolor>=1.1\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting termcolor>=1.1\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (7.1.0)\u001b[0m\n",
      "\u001b[34mCollecting yacs>=0.1.6\n",
      "  Downloading yacs-0.1.6-py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (4.42.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.2.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting fvcore\n",
      "  Downloading fvcore-0.1.dev200325.tar.gz (30 kB)\u001b[0m\n",
      "\u001b[35m  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (7.1.0)\u001b[0m\n",
      "\u001b[35mCollecting yacs>=0.1.6\n",
      "  Downloading yacs-0.1.6-py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (3.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (4.42.1)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard\n",
      "  Downloading tensorboard-2.2.0-py3-none-any.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (0.17.1)\u001b[0m\n",
      "\u001b[34mCollecting pydot\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from yacs>=0.1.6->detectron2==0.1.1->-r requirements.txt (line 5)) (5.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.11.3)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (0.34.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.13.1-py2.py3-none-any.whl (87 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[35mCollecting fvcore\n",
      "  Downloading fvcore-0.1.dev200325.tar.gz (30 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from detectron2==0.1.1->-r requirements.txt (line 5)) (0.17.1)\u001b[0m\n",
      "\u001b[35mCollecting pydot\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from yacs>=0.1.6->detectron2==0.1.1->-r requirements.txt (line 5)) (5.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (1.16.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (0.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.1.1->-r requirements.txt (line 5)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.14.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\u001b[0m\n",
      "\u001b[35mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.11.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.14.0)\u001b[0m\n",
      "\u001b[35mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[35mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.13.1-py2.py3-none-any.whl (87 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (0.34.2)\u001b[0m\n",
      "\u001b[35mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting portalocker\n",
      "  Downloading portalocker-1.6.0-py2.py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, detectron2, termcolor, fvcore, absl-py\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[35mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2.22.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\u001b[0m\n",
      "\u001b[35mCollecting portalocker\n",
      "  Downloading portalocker-1.6.0-py2.py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.4.2)\u001b[0m\n",
      "\u001b[35mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (1.25.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2019.11.28)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (2.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1->-r requirements.txt (line 5)) (0.4.8)\u001b[0m\n",
      "\u001b[35mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name, detectron2, termcolor, fvcore, absl-py\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=15662007 sha256=8b0c1b4135a3bd7ac9ca8ca9bf64ba84d3a697458fd07c580ed9c4af0c150681\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6o34t2is/wheels/d0/81/da/71ed4038f5444cd155ae7979510c543208fbd312baf06d4e5f\n",
      "  Building wheel for detectron2 (setup.py): started\u001b[0m\n",
      "\u001b[35m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=15662005 sha256=ac5c5405c7ef37456514646d841dd2e859b79b49c76d6c88b913487674d7a109\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pojg6z1z/wheels/23/0a/4c/a6cbd03e93107a45d1fd878311050888ee409d65521645426e\n",
      "  Building wheel for detectron2 (setup.py): started\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for detectron2 (setup.py): finished with status 'done'\n",
      "  Created wheel for detectron2: filename=detectron2-0.1.1-cp36-cp36m-linux_x86_64.whl size=4988221 sha256=89c8d9781224b4bd0f27767a0fd1ef65dd9714cb7758799fa69b6f9e32a2d1c0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pojg6z1z/wheels/2f/3b/c1/00424ff1494f5bd9f617a9b97453ad806595044a9cef3081da\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=c7652f5a8f84d5dbc89a2451356eb49635087fb5f4150be21217dd8af43af7d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for fvcore (setup.py): started\u001b[0m\n",
      "\u001b[35m  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.dev200325-py3-none-any.whl size=38930 sha256=e58988bbd17f0734bcd5c47a0d9dac7eb479d31c3bf1a6df7c145cdc2326296d\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/3c/9b/22c6bb909ae65fea9d4cf3d6907bfbd6bcadde03c1ed511163\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=924dd6336a99030a9f09ebf7de28fa8344378af710b4aff79805447d1df74b5a\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name detectron2 termcolor fvcore absl-py\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name, termcolor, yacs, tabulate, grpcio, markdown, pyasn1-modules, cachetools, google-auth, absl-py, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, portalocker, fvcore, pydot, detectron2\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-0.9.0 cachetools-4.0.0 default-user-module-name-1.0.0 detectron2-0.1.1 fvcore-0.1.dev200325 google-auth-1.13.1 google-auth-oauthlib-0.4.1 grpcio-1.28.1 markdown-3.2.1 oauthlib-3.1.0 portalocker-1.6.0 pyasn1-modules-0.2.8 pydot-1.4.1 requests-oauthlib-1.3.0 tabulate-0.8.7 tensorboard-2.2.0 tensorboard-plugin-wit-1.6.0.post3 termcolor-1.1.0 yacs-0.1.6\u001b[0m\n",
      "\u001b[35m2020-04-06 20:38:32,190 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-04-06-20-25-34-343\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-553020858742/pytorch-training-2020-04-06-20-25-34-343/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-2-553020858742/pytorch-training-2020-04-06-20-25-34-343/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-04-06-20-25-34-343\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-553020858742/pytorch-training-2020-04-06-20-25-34-343/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python train.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mCollecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
      "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-oke1q4r3\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.6/site-packages (from pycocotools==2.0) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.6/site-packages (from pycocotools==2.0) (0.29.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.6/site-packages (from pycocotools==2.0) (3.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.16.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.14.0)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): still running...\u001b[0m\n",
      "\u001b[35m  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=288479 sha256=512e805e8bfd741778e92d1f496b8f49fb9cd064a6a5ec4a588417d21e140374\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fss3c8ay/wheels/25/c1/63/8bee2969883497d2785c9bdbe4e89cae5efc59521553d528bf\u001b[0m\n",
      "\u001b[35mSuccessfully built pycocotools\u001b[0m\n",
      "\u001b[35mInstalling collected packages: pycocotools\u001b[0m\n",
      "\u001b[35mSuccessfully installed pycocotools-2.0\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:46 d2.engine.defaults]: #033[0mModel:\u001b[0m\n",
      "\u001b[35mGeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[35m)\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:48 d2.data.build]: #033[0mRemoved 0 images with no usable annotations. 61 images left.\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:48 d2.data.build]: #033[0mDistribution of instances among all 1 categories:\u001b[0m\n",
      "\u001b[35m#033[36m|  category  | #instances   |\u001b[0m\n",
      "\u001b[35m|:----------:|:-------------|\u001b[0m\n",
      "\u001b[35m|  balloon   | 255          |\u001b[0m\n",
      "\u001b[35m|            |              |#033[0m\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:48 d2.data.common]: #033[0mSerializing 61 elements to byte tensors and concatenating them all ...\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:48 d2.data.common]: #033[0mSerialized dataset takes 0.17 MiB\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:48 d2.data.detection_utils]: #033[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:48 d2.data.build]: #033[0mUsing training sampler TrainingSampler\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:57 d2.engine.train_loop]: #033[0mStarting training from iteration 0\u001b[0m\n",
      "\u001b[35m[2020-04-06 20:38:57.666 algo-2:479 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2020-04-06 20:38:57.667 algo-2:479 INFO hook.py:170] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2020-04-06 20:38:57.667 algo-2:479 INFO hook.py:215] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m#033[4m#033[5m#033[31mERROR#033[0m #033[32m[04/06 20:38:57 d2.engine.train_loop]: #033[0mException during training:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 132, in train\n",
      "    self.run_step()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 215, in run_step\n",
      "    loss_dict = self.model(data)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 531, in __call__\n",
      "    hook.register_hook(self)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 194, in register_hook\n",
      "    self.register_module(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 227, in register_module\n",
      "    self._backward_apply(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 186, in _backward_apply\n",
      "    param.register_hook(self.backward_hook(pname))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/tensor.py\", line 227, in register_hook\n",
      "    raise RuntimeError(\"cannot register a hook on a tensor that \"\u001b[0m\n",
      "\u001b[35mRuntimeError: cannot register a hook on a tensor that doesn't require gradient\u001b[0m\n",
      "\u001b[35m#033[32m[04/06 20:38:57 d2.engine.hooks]: #033[0mTotal training time: 0:00:00 (0:00:00 on hooks)\u001b[0m\n",
      "\u001b[35m[2020-04-06 20:38:57.708 algo-2:479 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[35m2020-04-06 20:38:58,034 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mCommand \"/opt/conda/bin/python train.py\"\n",
      "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-oke1q4r3\u001b[0m\n",
      "\u001b[35m#015model_final_f10217.pkl: 0.00B [00:00, ?B/s]#015model_final_f10217.pkl:   0%|          | 8.19k/178M [00:00<4:03:38, 12.2kB/s]#015model_final_f10217.pkl:   0%|          | 57.3k/178M [00:00<2:53:31, 17.1kB/s]#015model_final_f10217.pkl:   0%|          | 262k/178M [00:00<2:01:58, 24.3kB/s] #015model_final_f10217.pkl:   1%|          | 1.17M/178M [00:01<1:25:06, 34.6kB/s]#015model_final_f10217.pkl:   2%|         | 4.14M/178M [00:01<58:37, 49.4kB/s]  #015model_final_f10217.pkl:   4%|         | 7.99M/178M [00:01<40:09, 70.5kB/s]#015model_final_f10217.pkl:   6%|         | 11.3M/178M [00:01<27:36, 101kB/s] #015model_final_f10217.pkl:   8%|         | 15.0M/178M [00:01<18:55, 143kB/s]#015model_final_f10217.pkl:  10%|         | 18.1M/178M [00:01<13:01, 204kB/s]#015model_final_f10217.pkl:  11%|         | 19.6M/178M [00:02<09:06, 290kB/s]#015model_final_f10217.pkl:  13%|        | 23.5M/178M [00:02<06:14, 412kB/s]#015model_final_f10217.pkl:  15%|        | 26.2M/178M [00:02<04:19, 584kB/s]#015model_final_f10217.pkl:  16%|        | 29.1M/178M [00:02<03:00, 824kB/s]#015model_final_f10217.pkl:  19%|        | 33.0M/178M [00:02<02:04, 1.16MB/s]#015model_final_f10217.pkl:  20%|        | 35.9M/178M [00:02<01:27, 1.62MB/s]#015model_final_f10217.pkl:  22%|       | 39.8M/178M [00:02<01:01, 2.25MB/s]#015model_final_f10217.pkl:  24%|       | 43.5M/178M [00:03<00:43, 3.12MB/s]#015model_final_f10217.pkl:  26%|       | 45.8M/178M [00:03<00:31, 4.17MB/s]#015model_final_f10217.pkl:  27%|       | 48.2M/178M [00:03<00:23, 5.54MB/s]#015model_final_f10217.pkl:  29%|       | 51.5M/178M [00:03<00:17, 7.37MB/s]#015model_final_f10217.pkl:  30%|       | 53.9M/178M [00:03<00:13, 9.13MB/s]#015model_final_f10217.pkl:  32%|      | 56.3M/178M [00:03<00:10, 11.2MB/s]#015model_final_f10217.pkl:  33%|      | 58.7M/178M [00:03<00:08, 13.3MB/s]#015model_final_f10217.pkl:  34%|      | 61.2M/178M [00:03<00:07, 15.1MB/s]#015model_final_f10217.pkl:  36%|      | 64.4M/178M [00:03<00:06, 17.5MB/s]#015model_final_f10217.pkl:  38%|      | 67.1M/178M [00:04<00:06, 18.4MB/s]#015model_final_f10217.pkl:  40%|      | 70.5M/178M [00:04<00:05, 21.2MB/s]#015model_final_f10217.pkl:  41%|      | 73.3M/178M [00:04<00:04, 21.0MB/s]#015model_final_f10217.pkl:  43%|     | 76.7M/178M [00:04<00:04, 23.5MB/s]#015model_final_f10217.pkl:  45%|     | 79.4M/178M [00:04<00:04, 24.4MB/s]#015model_final_f10217.pkl:  46%|     | 81.5M/178M [00:04<00:04, 22.5MB/s]#015model_final_f10217.pkl:  48%|     | 84.7M/178M [00:04<00:03, 24.3MB/s]#015model_final_f10217.pkl:  49%|     | 86.9M/178M [00:04<00:04, 22.0MB/s]#015model_final_f10217.pkl:  50%|     | 89.8M/178M [00:04<00:03, 23.7MB/s]#015model_final_f10217.pkl:  52%|    | 92.8M/178M [00:04<00:03, 25.2MB/s]#015model_final_f10217.pkl:  53%|    | 95.1M/178M [00:05<00:03, 23.2MB/s]#015model_final_f10217.pkl:  55%|    | 97.8M/178M [00:05<00:03, 23.9MB/s]#015model_final_f10217.pkl:  57%|    | 101M/178M [00:05<00:02, 26.0MB/s] #015model_final_f10217.pkl:  58%|    | 103M/178M [00:05<00:03, 23.0MB/s]#015model_final_f10217.pkl:  60%|    | 106M/178M [00:05<00:02, 24.2MB/s]#015model_final_f10217.pkl:  61%|   | 109M/178M [00:05<00:02, 24.5MB/s]#015model_final_f10217.pkl:  63%|   | 112M/178M [00:05<00:02, 24.5MB/s]#015model_final_f10217.pkl:  65%|   | 115M/178M [00:05<00:02, 26.2MB/s]#015model_final_f10217.pkl:  66%|   | 117M/178M [00:05<00:02, 24.3MB/s]#015model_final_f10217.pkl:  68%|   | 120M/178M [00:06<00:02, 24.8MB/s]#015model_final_f10217.pkl:  69%|   | 123M/178M [00:06<00:02, 25.7MB/s]#015model_final_f10217.pkl:  71%|   | 125M/178M [00:06<00:02, 23.8MB/s]#015model_final_f10217.pkl:  72%|  | 128M/178M [00:06<00:02, 24.7MB/s]#015model_final_f10217.pkl:  74%|  | 131M/178M [00:06<00:01, 25.9MB/s]#015model_final_f10217.pkl:  75%|  | 133M/178M [00:06<00:01, 23.8MB/s]#015model_final_f10217.pkl:  77%|  | 136M/178M [00:06<00:01, 25.2MB/s]#015model_final_f10217.pkl:  78%|  | 139M/178M [00:06<00:01, 24.4MB/s]#015model_final_f10217.pkl:  80%|  | 142M/178M [00:06<00:01, 24.2MB/s]#015model_final_f10217.pkl:  81%|  | 144M/178M [00:07<00:01, 25.4MB/s]#015model_final_f10217.pkl:  82%| | 147M/178M [00:07<00:01, 24.1MB/s]#015model_final_f10217.pkl:  84%| | 150M/178M [00:07<00:01, 24.7MB/s]#015model_final_f10217.pkl:  85%| | 152M/178M [00:07<00:01, 24.4MB/s]#015model_final_f10217.pkl:  87%| | 155M/178M [00:07<00:00, 25.3MB/s]#015model_final_f10217.pkl:  89%| | 158M/178M [00:07<00:00, 24.5MB/s]#015model_final_f10217.pkl:  90%| | 161M/178M [00:07<00:00, 25.1MB/s]#015model_final_f10217.pkl:  92%|| 163M/178M [00:07<00:00, 25.9MB/s]#015model_final_f10217.pkl:  93%|| 166M/178M [00:07<00:00, 25.4MB/s]#015model_final_f10217.pkl:  95%|| 169M/178M [00:08<00:00, 25.4MB/s]#015model_final_f10217.pkl:  96%|| 171M/178M [00:08<00:00, 25.8MB/s]#015model_final_f10217.pkl:  98%|| 174M/178M [00:08<00:00, 25.1MB/s]#015model_final_f10217.pkl:  99%|| 177M/178M [00:08<00:00, 25.2MB/s]#015model_final_f10217.pkl: 178MB [00:08, 21.2MB/s]                           \u001b[0m\n",
      "\u001b[35m'roi_heads.box_predictor.cls_score.weight' has shape (81, 1024) in the checkpoint but (2, 1024) in the model! Skipped.\u001b[0m\n",
      "\u001b[35m'roi_heads.box_predictor.cls_score.bias' has shape (81,) in the checkpoint but (2,) in the model! Skipped.\u001b[0m\n",
      "\u001b[35m'roi_heads.box_predictor.bbox_pred.weight' has shape (320, 1024) in the checkpoint but (4, 1024) in the model! Skipped.\u001b[0m\n",
      "\u001b[35m'roi_heads.box_predictor.bbox_pred.bias' has shape (320,) in the checkpoint but (4,) in the model! Skipped.\u001b[0m\n",
      "\u001b[35m'roi_heads.mask_head.predictor.weight' has shape (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! Skipped.\u001b[0m\n",
      "\u001b[35m'roi_heads.mask_head.predictor.bias' has shape (80,) in the checkpoint but (1,) in the model! Skipped.\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"train.py\", line 102, in <module>\n",
      "    train()\n",
      "  File \"train.py\", line 49, in train\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/defaults.py\", line 380, in train\n",
      "    super().train(self.start_iter, self.max_iter)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 132, in train\n",
      "    self.run_step()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 215, in run_step\n",
      "    loss_dict = self.model(data)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 531, in __call__\n",
      "    hook.register_hook(self)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 194, in register_hook\n",
      "    self.register_module(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 227, in register_module\n",
      "    self._backward_apply(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 186, in _backward_apply\n",
      "    param.register_hook(self.backward_hook(pname))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/tensor.py\", line 227, in register_hook\n",
      "    raise RuntimeError(\"cannot register a hook on a tensor that \"\u001b[0m\n",
      "\u001b[35mRuntimeError: cannot register a hook on a tensor that doesn't require gradient\u001b[0m\n",
      "\u001b[34m  Building wheel for detectron2 (setup.py): finished with status 'done'\n",
      "  Created wheel for detectron2: filename=detectron2-0.1.1-cp36-cp36m-linux_x86_64.whl size=4995320 sha256=86a418ed64f0c537456ad9ad753ab5557762d51f7689e59f862c230371774335\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6o34t2is/wheels/2f/3b/c1/00424ff1494f5bd9f617a9b97453ad806595044a9cef3081da\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=26f35ece6d0ab57f7652fbaf2873a3e7b81ef28813bc52d1c44930817fd08d40\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.dev200325-py3-none-any.whl size=38930 sha256=9c30d781d6c18136fcabb36dec703ee96acdcbfda0e46c720bbdc1c0ea48b128\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/3c/9b/22c6bb909ae65fea9d4cf3d6907bfbd6bcadde03c1ed511163\n",
      "  Building wheel for absl-py (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=034337d8bb27bc31b9192b0bef392839ec8d4fa1b57d155bfb5fdbc059232ba8\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name detectron2 termcolor fvcore absl-py\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name, termcolor, yacs, tabulate, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, absl-py, tensorboard-plugin-wit, grpcio, tensorboard, portalocker, fvcore, pydot, detectron2\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-0.9.0 cachetools-4.0.0 default-user-module-name-1.0.0 detectron2-0.1.1 fvcore-0.1.dev200325 google-auth-1.13.1 google-auth-oauthlib-0.4.1 grpcio-1.28.1 markdown-3.2.1 oauthlib-3.1.0 portalocker-1.6.0 pyasn1-modules-0.2.8 pydot-1.4.1 requests-oauthlib-1.3.0 tabulate-0.8.7 tensorboard-2.2.0 tensorboard-plugin-wit-1.6.0.post3 termcolor-1.1.0 yacs-0.1.6\u001b[0m\n",
      "\u001b[34m2020-04-06 20:39:09,198 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-04-06-20-25-34-343\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-553020858742/pytorch-training-2020-04-06-20-25-34-343/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-553020858742/pytorch-training-2020-04-06-20-25-34-343/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-04-06-20-25-34-343\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-553020858742/pytorch-training-2020-04-06-20-25-34-343/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
      "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-hm5mns3_\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.6/site-packages (from pycocotools==2.0) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.6/site-packages (from pycocotools==2.0) (0.29.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.6/site-packages (from pycocotools==2.0) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.14.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=288484 sha256=0f234777f8dc239257eb8cc94eafd98a496dfe871fa21da6bacd4849eee1c282\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-820d36wl/wheels/25/c1/63/8bee2969883497d2785c9bdbe4e89cae5efc59521553d528bf\u001b[0m\n",
      "\u001b[34mSuccessfully built pycocotools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pycocotools\u001b[0m\n",
      "\u001b[34mSuccessfully installed pycocotools-2.0\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:23 d2.engine.defaults]: #033[0mModel:\u001b[0m\n",
      "\u001b[34mGeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:25 d2.data.build]: #033[0mRemoved 0 images with no usable annotations. 61 images left.\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:25 d2.data.build]: #033[0mDistribution of instances among all 1 categories:\u001b[0m\n",
      "\u001b[34m#033[36m|  category  | #instances   |\u001b[0m\n",
      "\u001b[34m|:----------:|:-------------|\u001b[0m\n",
      "\u001b[34m|  balloon   | 255          |\u001b[0m\n",
      "\u001b[34m|            |              |#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:25 d2.data.common]: #033[0mSerializing 61 elements to byte tensors and concatenating them all ...\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:25 d2.data.common]: #033[0mSerialized dataset takes 0.17 MiB\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:25 d2.data.detection_utils]: #033[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:25 d2.data.build]: #033[0mUsing training sampler TrainingSampler\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:33 d2.engine.train_loop]: #033[0mStarting training from iteration 0\u001b[0m\n",
      "\u001b[34m[2020-04-06 20:39:34.215 algo-1:479 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-04-06 20:39:34.216 algo-1:479 INFO hook.py:170] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-04-06 20:39:34.216 algo-1:479 INFO hook.py:215] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m#033[4m#033[5m#033[31mERROR#033[0m #033[32m[04/06 20:39:34 d2.engine.train_loop]: #033[0mException during training:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 132, in train\n",
      "    self.run_step()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 215, in run_step\n",
      "    loss_dict = self.model(data)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 531, in __call__\n",
      "    hook.register_hook(self)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 194, in register_hook\n",
      "    self.register_module(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 227, in register_module\n",
      "    self._backward_apply(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 186, in _backward_apply\n",
      "    param.register_hook(self.backward_hook(pname))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/tensor.py\", line 227, in register_hook\n",
      "    raise RuntimeError(\"cannot register a hook on a tensor that \"\u001b[0m\n",
      "\u001b[34mRuntimeError: cannot register a hook on a tensor that doesn't require gradient\u001b[0m\n",
      "\u001b[34m#033[32m[04/06 20:39:34 d2.engine.hooks]: #033[0mTotal training time: 0:00:00 (0:00:00 on hooks)\u001b[0m\n",
      "\u001b[34m[2020-04-06 20:39:34.251 algo-1:479 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-04-06 20:39:34,626 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python train.py\"\n",
      "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-hm5mns3_\u001b[0m\n",
      "\u001b[34m#015model_final_f10217.pkl: 0.00B [00:00, ?B/s]#015model_final_f10217.pkl:   0%|          | 8.19k/178M [00:00<3:45:43, 13.1kB/s]#015model_final_f10217.pkl:   0%|          | 57.3k/178M [00:00<2:40:46, 18.4kB/s]#015model_final_f10217.pkl:   0%|          | 262k/178M [00:00<1:53:05, 26.2kB/s] #015model_final_f10217.pkl:   1%|          | 1.17M/178M [00:01<1:18:54, 37.3kB/s]#015model_final_f10217.pkl:   2%|         | 4.00M/178M [00:01<54:23, 53.3kB/s]  #015model_final_f10217.pkl:   3%|         | 6.01M/178M [00:01<37:41, 76.0kB/s]#015model_final_f10217.pkl:   5%|         | 9.55M/178M [00:01<25:52, 108kB/s] #015model_final_f10217.pkl:   7%|         | 12.9M/178M [00:01<17:46, 155kB/s]#015model_final_f10217.pkl:   8%|         | 15.0M/178M [00:01<12:20, 220kB/s]#015model_final_f10217.pkl:  10%|         | 18.4M/178M [00:01<08:28, 313kB/s]#015model_final_f10217.pkl:  12%|        | 20.8M/178M [00:01<05:53, 445kB/s]#015model_final_f10217.pkl:  13%|        | 23.1M/178M [00:02<04:05, 630kB/s]#015model_final_f10217.pkl:  15%|        | 26.2M/178M [00:02<02:49, 892kB/s]#015model_final_f10217.pkl:  16%|        | 28.6M/178M [00:02<01:59, 1.25MB/s]#015model_final_f10217.pkl:  17%|        | 30.8M/178M [00:02<01:24, 1.75MB/s]#015model_final_f10217.pkl:  19%|        | 33.1M/178M [00:02<00:59, 2.42MB/s]#015model_final_f10217.pkl:  20%|        | 36.3M/178M [00:02<00:42, 3.32MB/s]#015model_final_f10217.pkl:  22%|       | 38.6M/178M [00:02<00:31, 4.46MB/s]#015model_final_f10217.pkl:  23%|       | 41.4M/178M [00:02<00:22, 5.96MB/s]#015model_final_f10217.pkl:  25%|       | 44.2M/178M [00:02<00:17, 7.76MB/s]#015model_final_f10217.pkl:  26%|       | 46.7M/178M [00:02<00:13, 9.77MB/s]#015model_final_f10217.pkl:  27%|       | 48.8M/178M [00:03<00:11, 11.6MB/s]#015model_final_f10217.pkl:  29%|       | 52.1M/178M [00:03<00:08, 14.2MB/s]#015model_final_f10217.pkl:  31%|       | 54.7M/178M [00:03<00:07, 15.4MB/s]#015model_final_f10217.pkl:  33%|      | 58.2M/178M [00:03<00:06, 18.4MB/s]#015model_final_f10217.pkl:  34%|      | 60.7M/178M [00:03<00:06, 18.8MB/s]#015model_final_f10217.pkl:  36%|      | 63.8M/178M [00:03<00:05, 21.3MB/s]#015model_final_f10217.pkl:  37%|      | 66.4M/178M [00:03<00:04, 22.5MB/s]#015model_final_f10217.pkl:  39%|      | 68.7M/178M [00:03<00:04, 21.9MB/s]#015model_final_f10217.pkl:  40%|      | 71.4M/178M [00:03<00:04, 23.3MB/s]#015model_final_f10217.pkl:  41%|      | 73.2M/178M [00:04<00:04, 20.9MB/s]#015model_final_f10217.pkl:  43%|     | 76.3M/178M [00:04<00:04, 22.5MB/s]#015model_final_f10217.pkl:  44%|     | 78.9M/178M [00:04<00:04, 23.5MB/s]#015model_final_f10217.pkl:  46%|     | 81.9M/178M [00:04<00:03, 24.9MB/s]#015model_final_f10217.pkl:  47%|     | 84.5M/178M [00:04<00:03, 23.7MB/s]#015model_final_f10217.pkl:  49%|     | 87.4M/178M [00:04<00:03, 24.9MB/s]#015model_final_f10217.pkl:  51%|     | 90.2M/178M [00:04<00:03, 23.8MB/s]#015model_final_f10217.pkl:  53%|    | 93.5M/178M [00:04<00:03, 25.3MB/s]#015model_final_f10217.pkl:  54%|    | 96.4M/178M [00:04<00:03, 24.9MB/s]#015model_final_f10217.pkl:  56%|    | 99.0M/178M [00:05<00:03, 25.2MB/s]#015model_final_f10217.pkl:  57%|    | 102M/178M [00:05<00:02, 26.2MB/s] #015model_final_f10217.pkl:  59%|    | 104M/178M [00:05<00:03, 23.9MB/s]#015model_final_f10217.pkl:  60%|    | 107M/178M [00:05<00:02, 25.2MB/s]#015model_final_f10217.pkl:  62%|   | 110M/178M [00:05<00:02, 26.4MB/s]#015model_final_f10217.pkl:  63%|   | 112M/178M [00:05<00:02, 24.3MB/s]#015model_final_f10217.pkl:  65%|   | 115M/178M [00:05<00:02, 24.8MB/s]#015model_final_f10217.pkl:  66%|   | 118M/178M [00:05<00:02, 25.8MB/s]#015model_final_f10217.pkl:  68%|   | 120M/178M [00:05<00:02, 23.6MB/s]#015model_final_f10217.pkl:  69%|   | 123M/178M [00:06<00:02, 24.2MB/s]#015model_final_f10217.pkl:  71%|   | 126M/178M [00:06<00:02, 24.0MB/s]#015model_final_f10217.pkl:  72%|  | 129M/178M [00:06<00:01, 24.9MB/s]#015model_final_f10217.pkl:  74%|  | 132M/178M [00:06<00:01, 24.6MB/s]#015model_final_f10217.pkl:  76%|  | 135M/178M [00:06<00:01, 24.8MB/s]#015model_final_f10217.pkl:  78%|  | 138M/178M [00:06<00:01, 24.6MB/s]#015model_final_f10217.pkl:  79%|  | 141M/178M [00:06<00:01, 25.0MB/s]#015model_final_f10217.pkl:  81%|  | 144M/178M [00:06<00:01, 24.1MB/s]#015model_final_f10217.pkl:  82%| | 146M/178M [00:06<00:01, 25.2MB/s]#015model_final_f10217.pkl:  84%| | 149M/178M [00:07<00:01, 23.8MB/s]#015model_final_f10217.pkl:  86%| | 152M/178M [00:07<00:01, 25.1MB/s]#015model_final_f10217.pkl:  87%| | 155M/178M [00:07<00:00, 24.7MB/s]#015model_final_f10217.pkl:  89%| | 158M/178M [00:07<00:00, 25.8MB/s]#015model_final_f10217.pkl:  90%| | 161M/178M [00:07<00:00, 25.6MB/s]#015model_final_f10217.pkl:  92%|| 163M/178M [00:07<00:00, 24.2MB/s]#015model_final_f10217.pkl:  93%|| 166M/178M [00:07<00:00, 25.0MB/s]#015model_final_f10217.pkl:  95%|| 169M/178M [00:07<00:00, 23.2MB/s]#015model_final_f10217.pkl:  97%|| 172M/178M [00:08<00:00, 25.0MB/s]#015model_final_f10217.pkl:  98%|| 175M/178M [00:08<00:00, 23.6MB/s]#015model_final_f10217.pkl: 178MB [00:08, 21.5MB/s]                           \u001b[0m\n",
      "\u001b[34m'roi_heads.box_predictor.cls_score.weight' has shape (81, 1024) in the checkpoint but (2, 1024) in the model! Skipped.\u001b[0m\n",
      "\u001b[34m'roi_heads.box_predictor.cls_score.bias' has shape (81,) in the checkpoint but (2,) in the model! Skipped.\u001b[0m\n",
      "\u001b[34m'roi_heads.box_predictor.bbox_pred.weight' has shape (320, 1024) in the checkpoint but (4, 1024) in the model! Skipped.\u001b[0m\n",
      "\u001b[34m'roi_heads.box_predictor.bbox_pred.bias' has shape (320,) in the checkpoint but (4,) in the model! Skipped.\u001b[0m\n",
      "\u001b[34m'roi_heads.mask_head.predictor.weight' has shape (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! Skipped.\u001b[0m\n",
      "\u001b[34m'roi_heads.mask_head.predictor.bias' has shape (80,) in the checkpoint but (1,) in the model! Skipped.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 102, in <module>\n",
      "    train()\n",
      "  File \"train.py\", line 49, in train\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/defaults.py\", line 380, in train\n",
      "    super().train(self.start_iter, self.max_iter)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 132, in train\n",
      "    self.run_step()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/detectron2/engine/train_loop.py\", line 215, in run_step\n",
      "    loss_dict = self.model(data)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 531, in __call__\n",
      "    hook.register_hook(self)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 194, in register_hook\n",
      "    self.register_module(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 227, in register_module\n",
      "    self._backward_apply(module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/smdebug/pytorch/hook.py\", line 186, in _backward_apply\n",
      "    param.register_hook(self.backward_hook(pname))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/tensor.py\", line 227, in register_hook\n",
      "    raise RuntimeError(\"cannot register a hook on a tensor that \"\u001b[0m\n",
      "\u001b[34mRuntimeError: cannot register a hook on a tensor that doesn't require gradient\u001b[0m\n",
      "\n",
      "2020-04-06 20:39:54 Uploading - Uploading generated training model\n",
      "2020-04-06 20:39:54 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2020-04-06-20-25-34-343: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py\"\n  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-oke1q4r3\n\rmodel_final_f10217.pkl: 0.00B [00:00, ?B/s]\rmodel_final_f10217.pkl:   0%|          | 8.19k/178M [00:00<4:03:38, 12.2kB/s]\rmodel_final_f10217.pkl:   0%|          | 57.3k/178M [00:00<2:53:31, 17.1kB/s]\rmodel_final_f10217.pkl:   0%|          | 262k/178M [00:00<2:01:58, 24.3kB/s] \rmodel_final_f10217.pkl:   1%|          | 1.17M/178M [00:01<1:25:06, 34.6kB/s]\rmodel_final_f10217.pkl:   2%|         | 4.14M/178M [00:01<58:37, 49.4kB/s]  \rmodel_final_f10217.pkl:   4%|         | 7.99M/178M [00:01<40:09, 70.5kB/s]\rmodel_final_f10217.pkl:   6%|         | 11.3M/178M [00:01<27:36, 101kB/s] \rmodel_final_f10217.pkl:   8%|         | 15.0M/178M [00:01<18:55, 143kB/s]\rmodel_final_f10217.pkl:  10%|         | 18.1M/178M [00:01<13:01, 204kB/s]\rmodel_final_f10217.pkl:  11%|         | 19.6M/178M [00:02<09:06, 290kB/s]\rmodel_final_f1021",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f8558cf8f76d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m d2.fit({'train':\"s3://sagemaker-us-east-2-553020858742/balloon/train\",\n\u001b[0;32m---> 15\u001b[0;31m         'val':\"s3://sagemaker-us-east-2-553020858742/balloon/val\"})\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3024\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2615\u001b[0m                 ),\n\u001b[1;32m   2616\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2617\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2618\u001b[0m             )\n\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2020-04-06-20-25-34-343: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py\"\n  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-oke1q4r3\n\rmodel_final_f10217.pkl: 0.00B [00:00, ?B/s]\rmodel_final_f10217.pkl:   0%|          | 8.19k/178M [00:00<4:03:38, 12.2kB/s]\rmodel_final_f10217.pkl:   0%|          | 57.3k/178M [00:00<2:53:31, 17.1kB/s]\rmodel_final_f10217.pkl:   0%|          | 262k/178M [00:00<2:01:58, 24.3kB/s] \rmodel_final_f10217.pkl:   1%|          | 1.17M/178M [00:01<1:25:06, 34.6kB/s]\rmodel_final_f10217.pkl:   2%|         | 4.14M/178M [00:01<58:37, 49.4kB/s]  \rmodel_final_f10217.pkl:   4%|         | 7.99M/178M [00:01<40:09, 70.5kB/s]\rmodel_final_f10217.pkl:   6%|         | 11.3M/178M [00:01<27:36, 101kB/s] \rmodel_final_f10217.pkl:   8%|         | 15.0M/178M [00:01<18:55, 143kB/s]\rmodel_final_f10217.pkl:  10%|         | 18.1M/178M [00:01<13:01, 204kB/s]\rmodel_final_f10217.pkl:  11%|         | 19.6M/178M [00:02<09:06, 290kB/s]\rmodel_final_f1021"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "d2 = PyTorch(entry_point=\"train.py\", source_dir=\"d2_script\",\n",
    "             train_instance_count=2,\n",
    "             role=role,\n",
    "             train_instance_type='ml.p3.2xlarge',\n",
    "             framework_version=\"1.4.0\")\n",
    "\n",
    "#d2.set_hyperparameters(num_epochs = 1, num_classes = 2, )\n",
    "\n",
    "d2.fit({'train':\"s3://sagemaker-us-east-2-553020858742/balloon/train\",\n",
    "        'val':\"s3://sagemaker-us-east-2-553020858742/balloon/val\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
