{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectron2 on SKU-110K dataset\n",
    "\n",
    "** Index **\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"sagemaker-sku110k-dataset\" # \"YOUR-BUCKET\"\n",
    "prefix_data = \"detectron2/data\"\n",
    "prefix_model = \"detectron2/training_artefacts\"\n",
    "local_folder = \"cache\"\n",
    "\n",
    "sm_session = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib import request\n",
    "import tarfile\n",
    "from typing import Sequence, Mapping, Optional\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download SKU-110K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_dataset = (\"SKU110K_fixed\", \"http://trax-geometry.s3.amazonaws.com/cvpr_challenge/SKU110K_fixed.tar.gz\")\n",
    "\n",
    "if not (Path(local_folder) / sku_dataset[0]).exists():\n",
    "    compressed_file = tarfile.open(fileobj=request.urlopen(sku_dataset[1]), mode=\"r|gz\")\n",
    "    compressed_file.extractall(path=local_folder)\n",
    "else:\n",
    "    print(f\"Using the data in `{local_folder}` folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganize images\n",
    "\n",
    "Images are moved to three channels, training, validation and test, according to the prefix of the of the image name. The images are then uploaded to the S3 bucket specified in the setup.\n",
    "\n",
    ":warning: upload to S3 will take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = Path(local_folder) / sku_dataset[0] / \"images\"\n",
    "assert path_images.exists(), f\"{path_images} not found\"\n",
    "\n",
    "prefix_to_channel = {\n",
    "    \"train\": \"training\",\n",
    "    \"val\": \"validation\",\n",
    "    \"test\": \"test\",\n",
    "}\n",
    "for channel_name in prefix_to_channel.values():\n",
    "    if not (path_images.parent / channel_name).exists():\n",
    "        (path_images.parent / channel_name).mkdir()\n",
    "\n",
    "for path_img in path_images.iterdir():\n",
    "    for prefix in prefix_to_channel:\n",
    "        if path_img.name.startswith(prefix):\n",
    "            path_img.replace(path_images.parent / prefix_to_channel[prefix] / path_img.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detectron2 uses Pillow to read images. We found out that some images in the SKU dataset are corrupted, which causes the dataloader to raise an IOError exception. Therefore, we remove them from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTED_IMAGES = {\n",
    "    \"training\": (\"train_4222.jpg\", \"train_5822.jpg\", \"train_882.jpg\", \"train_924.jpg\"),\n",
    "    \"validation\": tuple(),\n",
    "    \"test\": (\"test_274.jpg\",)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel_name in prefix_to_channel.values():\n",
    "    for img_name in CORRUPTED_IMAGES[channel_name]:\n",
    "        try:\n",
    "            (path_images.parent / channel_name / img_name).unlink()\n",
    "            print(f\"{img_name} removed from channel {channel_name} \")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{img_name} not in channel {channel_name}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel_name in prefix_to_channel.values():\n",
    "    print(\n",
    "        f\"Number of {channel_name} images = {sum(1 for x in (path_images.parent / channel_name).glob('*.jpg'))}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_to_s3_imgs = {}\n",
    "\n",
    "for channel_name in prefix_to_channel.values():\n",
    "    inputs = sm_session.upload_data(\n",
    "        path=str(path_images.parent / channel_name) ,\n",
    "        bucket=bucket,\n",
    "        key_prefix=f\"{prefix_data}/{channel_name}\"\n",
    "    )\n",
    "    print(f\"{channel_name} images uploaded to {inputs}\")\n",
    "    channel_to_s3_imgs[channel_name] = inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations processing\n",
    "\n",
    "The annotations are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation_channel(\n",
    "    channel_id: str, path_to_annotation: Path, bucket_name: str, data_prefix: str,\n",
    "    img_annotation_to_ignore: Optional[Sequence[str]] = None\n",
    ") -> Sequence[Mapping]:\n",
    "    r\"\"\"Change format from original to augmented manifest files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    channel_id : str\n",
    "        name of the channel, i.e. training, validation or test\n",
    "    path_to_annotation : Path\n",
    "        path to annotation file\n",
    "    bucket_name : str\n",
    "        bucket where the data are uploaded\n",
    "    data_prefix : str\n",
    "        bucket prefix\n",
    "    img_annotation_to_ignore : Optional[Sequence[str]]\n",
    "        annotation from these images are ignore because the corresponding images are corrupted, default to None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequence[Mapping]\n",
    "        List of json lines, each lines contains the annotations for a single. This recreates the\n",
    "        format of augmented manifest files that are generated by Amazon SageMaker GroundTruth\n",
    "        labeling jobs\n",
    "    \"\"\"\n",
    "    if channel_id not in (\"training\", \"validation\", \"test\"):\n",
    "        raise ValueError(\n",
    "            f\"Channel identifier must be training, validation or test. The passed values is {channel_id}\"\n",
    "        )\n",
    "    if not path_to_annotation.exists():\n",
    "        raise FileNotFoundError(f\"Annotation file {path_to_annotation} not found\")\n",
    "\n",
    "    df_annotation = pd.read_csv(\n",
    "        path_to_annotation,\n",
    "        header=0,\n",
    "        names=(\n",
    "            \"image_name\",\n",
    "            \"x1\",\n",
    "            \"y1\",\n",
    "            \"x2\",\n",
    "            \"y2\",\n",
    "            \"class\",\n",
    "            \"image_width\",\n",
    "            \"image_height\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    df_annotation[\"left\"] = df_annotation[\"x1\"]\n",
    "    df_annotation[\"top\"] = df_annotation[\"y1\"]\n",
    "    df_annotation[\"width\"] = df_annotation[\"x2\"] - df_annotation[\"x1\"]\n",
    "    df_annotation[\"height\"] = df_annotation[\"y2\"] - df_annotation[\"y1\"]\n",
    "    df_annotation.drop(columns=[\"x1\", \"x2\", \"y1\", \"y2\"], inplace=True)\n",
    "\n",
    "    jsonlines = []\n",
    "    for img_id in df_annotation[\"image_name\"].unique():\n",
    "        if img_annotation_to_ignore and img_id in img_annotation_to_ignore:\n",
    "            print(f\"Annotations for image {img_id} are neglected as the image is corrupted\")\n",
    "            continue\n",
    "        img_annotations = df_annotation.loc[df_annotation[\"image_name\"] == img_id, :]\n",
    "        annotations = []\n",
    "        for (\n",
    "            _,\n",
    "            _,\n",
    "            img_width,\n",
    "            img_heigh,\n",
    "            bbox_l,\n",
    "            bbox_t,\n",
    "            bbox_w,\n",
    "            bbox_h,\n",
    "        ) in img_annotations.itertuples(index=False):\n",
    "            annotations.append(\n",
    "                {\n",
    "                    \"class_id\": 0,\n",
    "                    \"width\": bbox_w,\n",
    "                    \"top\": bbox_t,\n",
    "                    \"left\": bbox_l,\n",
    "                    \"height\": bbox_h,\n",
    "                }\n",
    "            )\n",
    "        jsonline = {\n",
    "            \"sku\": {\n",
    "                \"annotations\": annotations,\n",
    "                \"image_size\": [{\"width\": img_width, \"depth\": 3, \"height\": img_heigh,}],\n",
    "            },\n",
    "            \"sku-metadata\": {\n",
    "                \"job_name\": f\"labeling-job/sku-110k-{channel_id}\",\n",
    "                \"class-map\": {\"0\": \"SKU\"},\n",
    "                \"human-annotated\": \"yes\",\n",
    "                \"objects\": len(annotations) * [{\"confidence\": 0.0}],\n",
    "                \"type\": \"groundtruth/object-detection\",\n",
    "                \"creation-date\": datetime.now()\n",
    "                .replace(second=0, microsecond=0)\n",
    "                .isoformat(),\n",
    "            },\n",
    "            \"source-ref\": f\"s3://{bucket_name}/{data_prefix}/{channel_id}/{img_id}\",\n",
    "        }\n",
    "        jsonlines.append(jsonline)\n",
    "    return jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_to_annotation_path = {\n",
    "    \"training\": Path(local_folder) / sku_dataset[0] / \"annotations\" / \"annotations_train.csv\",\n",
    "    \"validation\": Path(local_folder) / sku_dataset[0] / \"annotations\" / \"annotations_val.csv\",\n",
    "    \"test\": Path(local_folder) / sku_dataset[0] / \"annotations\" / \"annotations_test.csv\",\n",
    "}\n",
    "channel_to_annotation = {}\n",
    "\n",
    "for channel in channel_to_annotation_path:\n",
    "    annotations = create_annotation_channel(\n",
    "        channel,\n",
    "        channel_to_annotation_path[channel],\n",
    "        bucket,\n",
    "        prefix_data,\n",
    "        CORRUPTED_IMAGES[channel]\n",
    "    )\n",
    "    print(f\"Number of {channel} annotations: {len(annotations)}\")\n",
    "    channel_to_annotation[channel] = annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_annotations(p_annotations, p_channel: str):\n",
    "    rsc_bucket = boto3.resource(\"s3\").Bucket(bucket)\n",
    "    \n",
    "    json_lines = [json.dumps(elem) for elem in p_annotations]\n",
    "    to_write = \"\\n\".join(json_lines)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\") as fid:\n",
    "        fid.write(to_write)\n",
    "        rsc_bucket.upload_file(fid.name, f\"{prefix_data}/annotations/{p_channel}.manifest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel_id, annotations in channel_to_annotation.items():\n",
    "    upload_annotations(annotations, channel_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
